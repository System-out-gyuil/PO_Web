import os
import re
import requests
from datetime import datetime
from django.core.management.base import BaseCommand
from board.models import BizInfo
from main.models import Industry
from PO.management.commands.utils import fetch_iframe_src
from config import BIZINFO_API_KEY, CHROME_DRIVER_PATH, OPEN_AI_API_KEY, NAVER_CLOVA_OCR_API_KEY, NAVER_CLOUD_CLOVA_OCR_API_URL, ES_API_KEY
from langchain_openai import ChatOpenAI
import pdfplumber
import uuid
import json
import time
from PIL import Image
import subprocess
import warnings
warnings.filterwarnings("ignore", category=UserWarning)  # ê²½ê³  ë¬´ì‹œ
import pandas as pd

class Command(BaseCommand):
    help = "DB ì—…ë°ì´íŠ¸"

    def handle(self, *args, **kwargs):
        url = "https://www.bizinfo.go.kr/uss/rss/bizinfoApi.do"
        params = {
            "crtfcKey": BIZINFO_API_KEY,
            "dataType": "json",
            "searchCnt": 1020, # ì¡°íšŒí•  ì „ì²´ ê°œìˆ˜
            "pageUnit": 204, # í˜ì´ì§€ë‹¹ ê°œìˆ˜
            "pageIndex": 4 # í˜ì´ì§€ ë²ˆí˜¸
        }

        try:
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            items = response.json().get("jsonArray", [])

            for item in items:
                pblanc_id = item.get("pblancId")
                if BizInfo.objects.filter(pblanc_id=pblanc_id).exists():
                    continue

                reception_start = datetime.strptime("19000101", "%Y%m%d").date()
                reception_end = datetime.strptime("99991231", "%Y%m%d").date()
                reception_raw = item.get("reqstBeginEndDe")
                if reception_raw and "~" in reception_raw:
                    try:
                        start_str, end_str = reception_raw.split("~")
                        reception_start = datetime.strptime(start_str.strip(), "%Y%m%d").date()
                        reception_end = datetime.strptime(end_str.strip(), "%Y%m%d").date()
                    except:
                        pass

                creatPnttm = item.get("creatPnttm")
                registered_at = datetime.strptime(creatPnttm, "%Y-%m-%d %H:%M:%S").date() if creatPnttm else None
                iframe_src = fetch_iframe_src(pblanc_id, CHROME_DRIVER_PATH)

                file_url = item.get("printFlpthNm")
                raw_file_name = item.get("printFileNm") or "default.pdf"
                file_name = self.sanitize_filename(raw_file_name)
                text, structured_data = "", {}
                if file_url:
                    try:
                        file_path = self.download_file(file_url, file_name)
                        text, extra_path = self.extract_text(file_path)
                        structured_data = self.extract_structured_data(text)
                        print("\nğŸ“„ structured_data:", structured_data)
                        if os.path.exists(file_path):
                            os.remove(file_path)
                        if extra_path and os.path.exists(extra_path):
                            os.remove(extra_path)
                    except Exception as e:
                        self.stderr.write(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_url} - {e}")

                # MySQLì— ì €ì¥
                BizInfo.objects.create(
                    pblanc_id=pblanc_id,
                    title=item.get("pblancNm"),
                    content=item.get("bsnsSumryCn"),
                    registered_at=registered_at,
                    reception_start=reception_start,
                    reception_end=reception_end,
                    institution_name=item.get("jrsdInsttNm"),
                    enroll_method=item.get("reqstMthPapersCn") or "ì‹ ì²­ ë°©ë²•ì€ ê³µê³ ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.",
                    target=item.get("trgetNm"),
                    field=item.get("pldirSportRealmLclasCodeNm"),
                    hashtag=item.get("hashtags"),
                    print_file_name=raw_file_name,
                    print_file_path=item.get("printFlpthNm"),
                    company_hall_path=item.get("pblancUrl"),
                    support_field=item.get("pldirSportRealmMlsfcCodeNm"),
                    application_form_name=item.get("fileNm") or "",
                    application_form_path=item.get("flpthNm") or "",
                    iframe_src=iframe_src,
                    employee_count=structured_data.get("ì§ì›ìˆ˜", "test"),
                    revenue=structured_data.get("ë§¤ì¶œê·œëª¨", "test"),
                    noti_summary=structured_data.get("ê³µê³ ë‚´ìš©"),
                    business_period=structured_data.get("ì‚¬ì—…ê¸°ê°„(ì—…ë ¥)", "test"),
                    region=structured_data.get("ì§€ì—­"),
                    possible_industry=structured_data.get("ê°€ëŠ¥ì—…ì¢…"),
                    export_performance=structured_data.get("ìˆ˜ì¶œì‹¤ì ì—¬ë¶€", "test")
                )

            self.stdout.write(self.style.SUCCESS(f"{len(items)}ê±´ ì²˜ë¦¬ ì™„ë£Œ."))

            # Elasticsearchì— ì €ì¥
            self.es_indexing()

        except Exception as e:
            self.stderr.write(self.style.ERROR(f"ì‹¤íŒ¨: {e}"))

    def sanitize_filename(self, name):
        return re.sub(r"[^\wê°€-í£._]+", "_", name).strip("_")

    def download_file(self, url, file_name):
        response = requests.get(url, stream=True, timeout=15)
        response.raise_for_status()

        save_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "files"))
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, file_name)

        with open(save_path, "wb") as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)
        print("===============================================================================================\n ğŸ“‚ save_path:", save_path)
        return save_path

    def is_text_pdf(self, file_path):
        try:

            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages[:2]:
                    if page.extract_text():
                        return True
            return False
        except:
            return False

    def extract_text(self, file_path):
        full_text = ""
        extra_path = None

        if file_path.endswith(".pdf"):
            if self.is_text_pdf(file_path):
                with pdfplumber.open(file_path) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            full_text += page_text + "\n"
                return full_text.strip(), None
            else:
                return self.clova_ocr(file_path, "pdf"), None

        elif file_path.endswith((".jpg", ".jpeg", ".png")):
            return self.clova_ocr(file_path, "jpg"), None

        elif file_path.endswith(".hwp"):
            pdf_path = self.convert_hwp_to_pdf(file_path)
            if os.path.exists(pdf_path):
                extra_path = pdf_path
                extracted_text, _ = self.extract_text(pdf_path)
                return extracted_text, extra_path
            else:
                return "ì˜¤ë¥˜", None

        return full_text.strip() or "ì˜¤ë¥˜", None

    def clova_ocr(self, file_path, fmt):
        request_json = {
            'images': [{'format': fmt, 'name': 'demo'}],
            'requestId': str(uuid.uuid4()),
            'version': 'V1',
            'timestamp': int(time.time() * 1000)
        }
        payload = {'message': json.dumps(request_json).encode('UTF-8')}
        files = [('file', open(file_path, 'rb'))]
        headers = {'X-OCR-SECRET': NAVER_CLOVA_OCR_API_KEY}
        response = requests.post(NAVER_CLOUD_CLOVA_OCR_API_URL, headers=headers, data=payload, files=files)

        full_text = ""
        for field in response.json()['images'][0].get('fields', []):
            full_text += field['inferText'] + " "
        return full_text.strip()

    def convert_hwp_to_pdf(self, hwp_path):
        output_dir = os.path.dirname(hwp_path)
        try:
            result = subprocess.run([
                "libreoffice",
                "--headless",
                "--convert-to", "pdf:writer_pdf_Export",
                hwp_path,
                "--outdir", output_dir
            ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=30)

            print("ğŸ–¥ï¸ libreoffice stdout:", result.stdout.decode())

            basename = os.path.splitext(os.path.basename(hwp_path))[0] + ".pdf"
            converted_pdf = os.path.join(output_dir, basename)

            if os.path.exists(converted_pdf):
                return converted_pdf
            else:
                print(f"[âŒ ë³€í™˜ ì‹¤íŒ¨] {converted_pdf} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return ""
        except Exception as e:
            print(f"[ì˜ˆì™¸ ë°œìƒ] HWP â†’ PDF ë³€í™˜ ì‹¤íŒ¨: {e}")
            return ""

    def extract_structured_data(self, text):
        prompt = (
                "ì•„ë˜ í…ìŠ¤íŠ¸ëŠ” ì •ë¶€ ì§€ì›ì‚¬ì—… ê³µê³ ë¬¸ì—ì„œ ì¶”ì¶œëœ ì‹¤ì œ ë‚´ìš©ì…ë‹ˆë‹¤.\n"
                "ì´ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ **ì›ë¬¸ì— ëª…ì‹œëœ ë‚´ìš©ë§Œìœ¼ë¡œ ì—„ê²©íˆ íŒë‹¨í•˜ì—¬**, **ì ˆëŒ€ë¡œ ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì„ í•˜ì§€ ë§ˆì„¸ìš”.**\n"
                "â€» ë¹ˆ ê°’ ì—†ì´ ëª¨ë“  í•­ëª©ì„ ì±„ì›Œì•¼ í•˜ë©°.\n"
                "â€» ëª¨ë“  ì„ íƒì§€ëŠ” ë°˜ë“œì‹œ ì œê³µëœ í•­ëª© ì¤‘ì—ì„œë§Œ ê³ ë¥´ê³ , ì›ë¬¸ì— ëª…í™•íˆ ì–¸ê¸‰ë˜ì§€ ì•Šì€ í•­ëª©ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "â€» ì›ë¬¸ì— ê¸°ì¤€ì´ ëª…í™•í•˜ì§€ ì•Šì„ ê²½ìš°, 'ë³´ìˆ˜ì ìœ¼ë¡œ ì‹¤ì œ ì„ ì • ê°€ëŠ¥ì„±ì´ ë†’ì€' í•­ëª©ë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.\n\n"
                "ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ JSON, ë¹ˆ ê°’ í—ˆìš© ë¶ˆê°€):\n"
                "{\n"
                "  \"ì§€ì—­\": [\"ì „êµ­\"] ë˜ëŠ” [\"ì„œìš¸\",\"ê²½ê¸°\",\"ì¸ì²œ\",\"ê°•ì›\",\"ê²½ë¶\",\"ê²½ë‚¨\",\"ë¶€ì‚°\",\"ëŒ€êµ¬\",\"ëŒ€ì „\",\"ê´‘ì£¼\",\"ìš¸ì‚°\",\"ì„¸ì¢…\",\"ì¶©ë¶\",\"ì¶©ë‚¨\",\"ì „ë¶\",\"ì „ë‚¨\",\"ì œì£¼\"] ì¤‘ ì›ë¬¸ ê·¼ê±°ë¡œ ë³µìˆ˜ ì„ íƒ],\n"
                "  \"ì§ì›ìˆ˜\": [\"ì§ì› ì—†ìŒ\", \"1~4ì¸\",\"5ì¸ ì´ìƒ\"] ì„ íƒì§€ ì¤‘ ì‹¤ì œ ì„ ì • ê°€ëŠ¥ì„±ì´ ë†’ì€ ë²”ìœ„ë¥¼ ëª¨ë‘ ì„ íƒ,\n"
                "  \"ì‚¬ì—…ê¸°ê°„(ì—…ë ¥)\": [\"ì‚¬ì—…ì ë“±ë¡ ì „\",\"3ë…„ ë¯¸ë§Œ\",\"3ë…„ ì´ìƒ\"] ì„ íƒì§€ ì¤‘ ì„ ì • ê°€ëŠ¥ì„±ì´ ë†’ì€ êµ¬ê°„ì„ ë³µìˆ˜ ì„ íƒ,\n"
                "  \"ë§¤ì¶œê·œëª¨\": [\"ë§¤ì¶œ ì—†ìŒ\", \"1ì–µ ì´í•˜\",\"1~5ì–µ\",\"5~10ì–µ\",\"10~30ì–µ\",\"30ì–µ ì´ìƒ\"] ì„ íƒì§€ ì¤‘ ì„ ì • ê°€ëŠ¥ì„±ì´ ë†’ì€ êµ¬ê°„ì„ ë³µìˆ˜ ì„ íƒ,\n"
                "  \"ìˆ˜ì¶œì‹¤ì ì—¬ë¶€\": [\"ìˆ˜ì¶œ ì‹¤ì  ë³´ìœ \", \"ë¬´ê´€\"] ë°˜ë“œì‹œ ì˜ˆì‹œ ì¤‘ ì„ ì • ê°€ëŠ¥ì„±ì´ ë†’ì€ ì„ íƒì§€ë¥¼ í•˜ë‚˜ ì„ íƒ,\n"
                "  \"ê³µê³ ë‚´ìš©\": \"ì§€ì› ëª©ì , ëŒ€ìƒ, ê¸°ê°„, ë°©ë²•, ìë¶€ë‹´, ì„ ì • ì ˆì°¨, ì§€ì› í•œë„ ë° ì œí•œ ì‚¬í•­ ë“±ì„ ì¢…í•©í•˜ì—¬ 450ì ì´ìƒìœ¼ë¡œ ì •ë°€í•˜ê²Œ ìš”ì•½í•œ ë¬¸ì¥\"\n"
                "  \"ê°€ëŠ¥ì—…ì¢…\": [\"ë†ì—…, ì„ì—… ë° ì–´ì—…\", \"ê´‘ì—…\", \"ì œì¡°ì—…\", \"ì „ê¸°, ê°€ìŠ¤, ì¦ê¸° ë° ê³µê¸° ì¡°ì ˆ ê³µê¸‰ì—…\",\
                      \"ìˆ˜ë„, í•˜ìˆ˜ ë° íê¸°ë¬¼ ì²˜ë¦¬, ì›ë£Œ ì¬ìƒì—…\", \"ê±´ì„¤ì—…\", \"ë„ë§¤ ë° ì†Œë§¤ì—…\", \"ìš´ìˆ˜ ë° ì°½ê³ ì—…\", \"ìˆ™ë°• ë° ìŒì‹ì ì—…\",\
                        \"ì •ë³´í†µì‹ ì—…\", \"ê¸ˆìœµ ë° ë³´í—˜ì—…\", \"ë¶€ë™ì‚°ì—…\", \"ì „ë¬¸, ê³¼í•™ ë° ê¸°ìˆ  ì„œë¹„ìŠ¤ì—…\", \"ì‚¬ì—…ì‹œì„¤ ê´€ë¦¬, ì‚¬ì—… ì§€ì› ë° ì„ëŒ€ ì„œë¹„ìŠ¤ì—…\", \
                        \"êµìœ¡ì„œë¹„ìŠ¤ì—…\", \"ë³´ê±´ì—… ë° ì‚¬íšŒë³µì§€ ì„œë¹„ìŠ¤ì—…\", \"ì˜ˆìˆ  ìŠ¤í¬ì¸  ë° ì—¬ê°€ê´€ë ¨ ì„œë¹„ìŠ¤ì—…\", \"í˜‘íšŒ ë° ë‹¨ì²´, ìˆ˜ë¦¬ ë° ê¸°íƒ€ ê°œì¸ì„œë¹„ìŠ¤ì—…\"]\
                          ì„ íƒì§€ ì¤‘ ê³µê³ ë‚´ìš© ë‚´ ì„ ì • ê°€ëŠ¥í•œ ì—…ì¢…ì„ ëª…í™•í•˜ê²Œ ì„ íƒ, ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥\n"
                "}\n\n"
                "í•„ìˆ˜ ì¤€ìˆ˜ì‚¬í•­:\n"
                "- ëª¨ë“  í‚¤ì— ë°˜ë“œì‹œ í•˜ë‚˜ ì´ìƒì˜ ê°’ì„ ì±„ì›Œì•¼ í•˜ë©°, ë¹ˆ ë°°ì—´ ë˜ëŠ” ëˆ„ë½ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
                "- ì œê³µëœ ì„ íƒì§€ ì™¸ì˜ í•­ëª©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                "- ìˆ˜ì¶œì‹¤ì ì—¬ë¶€ ì™¸ ë‹¤ë¥¸í•­ëª©ì— ëŒ€í•´ 'ë¬´ê´€' ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                "- ê³µê³  ë‚´ìš©ì„ ì œì™¸í•œ ëª¨ë“  í•­ëª©ì€ ë°°ì—´ì˜ í˜•íƒœë¡œ ì¶œë ¥í•˜ì‹œì˜¤.\n"
                "- ì›ë¬¸ì— ì§ì ‘ ê·¼ê±°í•œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ê³ , ì ˆëŒ€ë¡œ ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì„ í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                "- ì›ë¬¸ì— ì—†ëŠ” ì¡°ê±´ì€ 'ë³´ìˆ˜ì 'ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ì‹¤ì œ ì„ ì • ê°€ëŠ¥ì„±ì´ ë†’ì€ ê¸°ì¤€ì„ ì„ íƒí•˜ì‹­ì‹œì˜¤.\n"
            ) + text


        llm = ChatOpenAI(temperature=0, model_name='gpt-4o', openai_api_key=OPEN_AI_API_KEY)
        try:
            response = llm.invoke(prompt)
            return self.clean_json_from_response(getattr(response, "content", "").strip())
        except Exception as e:
            print(f"[GPT ì˜¤ë¥˜] {e}")
            return {"ì§ì›ìˆ˜": "ì˜¤ë¥˜", "ë§¤ì¶œê·œëª¨": "ì˜¤ë¥˜", "ê³µê³ ë‚´ìš©": "ì˜¤ë¥˜"}

    def clean_json_from_response(self, content: str) -> dict:
        try:
            match = re.search(r"```(?:json)?\\s*(\{.*?\})\\s*```", content, re.DOTALL)
            if match:
                return json.loads(match.group(1))
            match2 = re.search(r"(\{.*?\})", content, re.DOTALL)
            if match2:
                return json.loads(match2.group(1))
            print("âš ï¸ JSON ë¸”ë¡ ì¶”ì¶œ ì‹¤íŒ¨")
            print("ğŸ“„ ì›ë³¸ content:", content)
            return {}
        except Exception as e:
            print(f"[JSON íŒŒì‹± ì˜¤ë¥˜] {e}")
            return {}

    def es_indexing(self):
        from django.forms.models import model_to_dict
        import ast
        from elasticsearch import Elasticsearch, helpers

        # âœ… Elasticsearch í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        es = Elasticsearch(
            "https://0e0f4480a93d4cb78455e070163e467d.us-central1.gcp.cloud.es.io:443",
            api_key=ES_API_KEY
        )

        index_name = "bizinfo_index"

        # âœ… ì¸ë±ìŠ¤ ì‚­ì œ ë° ì¬ìƒì„±
        if es.indices.exists(index=index_name):
            es.indices.delete(index=index_name)
            print(f"âŒ ê¸°ì¡´ ì¸ë±ìŠ¤ '{index_name}' ì‚­ì œ")

        # âœ… ì¸ë±ìŠ¤ ìƒì„± (edge_ngram ì„¤ì • í¬í•¨)
        es.indices.create(
            index=index_name,
            body={
                "settings": {
                    "analysis": {
                        "tokenizer": {
                            "edge_ngram_tokenizer": {
                                "type": "edge_ngram",
                                "min_gram": 1,
                                "max_gram": 30,
                                "token_chars": ["letter", "digit", "whitespace", "punctuation"]
                            }
                        },
                        "analyzer": {
                            "edge_ngram_analyzer": {
                                "type": "custom",
                                "tokenizer": "edge_ngram_tokenizer",
                                "filter": ["lowercase"]
                            }
                        }
                    }
                },
                "mappings": {
                    "properties": {
                        "title": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "edge_ngram_analyzer"},
                        "region_first": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "edge_ngram_analyzer"},
                        "noti_summary": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "edge_ngram_analyzer"},
                        "possible_industry": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "edge_ngram_analyzer"},
                        "institution_name": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "edge_ngram_analyzer"},
                        "target": {"type": "text", "analyzer": "edge_ngram_analyzer", "search_analyzer": "edge_ngram_analyzer"},
                        "registered_at": {"type": "date"},
                        "reception_start": {"type": "date"},
                        "reception_end": {"type": "date"}
                    }
                }
            }
        )
        print(f"âœ… ìƒˆ ì¸ë±ìŠ¤ '{index_name}' ìƒì„± ì™„ë£Œ")

        # âœ… Elasticsearchì— ì¸ë±ì‹±í•  ë°ì´í„° ì¤€ë¹„
        actions = []

        list_fields = ["region", "employee_count", "revenue", "export_performance", "possible_industry", "business_period"]

        for obj in BizInfo.objects.all():
            doc = model_to_dict(obj)

            # ë‚ ì§œ í•„ë“œ ISO ë³€í™˜
            for field in ["registered_at", "reception_start", "reception_end"]:
                if doc.get(field):
                    doc[field] = doc[field].isoformat()

            # ë¦¬ìŠ¤íŠ¸í˜• í•„ë“œ ì²˜ë¦¬
            for field in list_fields:
                value = doc.get(field)
                if value:
                    if isinstance(value, list):
                        doc[field] = value
                    elif isinstance(value, str) and value.startswith("[") and value.endswith("]"):
                        try:
                            parsed = ast.literal_eval(value)
                            doc[field] = parsed if isinstance(parsed, list) else [parsed]
                        except Exception as e:
                            print(f"âš ï¸ {field} íŒŒì‹± ì‹¤íŒ¨: {value} â†’ {e}")
                            doc[field] = [value]
                    else:
                        doc[field] = [value]
                else:
                    doc[field] = []

            # region_first í•„ë“œ ìƒì„±
            doc["region_first"] = doc["region"][0] if doc["region"] else ""

            actions.append({
                "_index": index_name,
                "_id": doc["pblanc_id"],
                "_source": doc
            })

        # âœ… Elasticsearchì— ì—…ë¡œë“œ
        helpers.bulk(es, actions)
        print(f"ğŸ‰ ì´ {len(actions)}ê°œ ë¬¸ì„œ Elasticsearch ì¸ë±ì‹± ì™„ë£Œ")



        
