# 2025.04.29 (화)

## 1. Django setting 완료 및 초안

- 로컬 MySQL 연동 (DB 명 : PO)
- 나에게 맞는 지원금 검색, 상담 신청으로 db에 저장
- 관리자 페이지에서 상담 신청 데이터 list up
- 디자인 초안에 맞게 퍼블리싱
- 시간이 세계 표준시 UTC로 되어있어서 Asia/Seoul로 TimeZone 변경

## 2. 앞으로 해야 할 일

- 디자인 마무리
- 관리자 페이지 페이징처리, 체크기능, 데이터 삭제(소프트인지 하드인지 정하기)
- 서버 배포(Django, MySQL, ElasticSearch)
- 데이터 크롤링, 추출, 변환, 적제(elasticsearch에 indexing) 자동화
- 입력하는 데이터(region, industry, counsel form) 유효성 검사
- 검색 필터링(지역, 업종 등)
- 카카오 로그인 및 공유하기

<br>

# 2025.04.30 (수)

## 1. 프로젝트 배포

- AWS EC2 세팅 및 인바운드 보안그룹 세팅
- models 점검
- config 파일로 API_KEY, 비밀번호 등 보안 관리

<br>

# 2025.05.01 (목)

## 1. 프로젝트 프로토타입 배포

- AWS EC2 서비스로 웹 배포
- AWS EC2 서비스로 MySQL 서버 사용
- Gabia에서 도메인 결제 (namatji.com) 및 Route53과 DNS, Nginx를 통해 탄력 IP 설정과 도메인 배포
- Elasticsearch cloud로 ES API를 사용해서 Elasticsearch 서버 구축
- 웹 페이지 상단 '나에게 맞는 지원금' 클릭 시 메인 화면으로 이동
- 전문가 상담 폼에서 입력 양식 변경 및 유효성 검사(입력 했는지 안했는지)
- 검색 결과 페이지에서 더보기, 접기 클릭 시 제대로 작동 안하던 버그 수정

## 2. 앞으로 해야 할 일

- 디자인 깔끔하게 만들기
- 웹버전 만들기
- ES 서버 비용 청구하기
- 개인정보 제공 동의받기, 개인정보 보호법 제 15조 ~ 17조 참고
- 개인정보 제공 동의 컬럼 추가, 회원정보 저장해야하는지 체크하기
- 소상공인 교효율 ~~ 데이터 최상단 노출되게 만들기
- 공고일자 최신순 정렬하기
- config파일 합치기, ubuntu 환경에서도 따로 변경해야함
- 구글 애드센스 더 알아보기
- 메인페이지에서 지역, 업종 둘 중 하나만 선택해도 검색 하도록 하기(?)

경험에 따른 데이터 처리..
선정될만한 기준
쿠팡파트너스, 구글 애드센스
https

<br>

# 2025.05.02 (금)

## 1. HTTPS
- certbot으로 시도했는데 다섯번 실패하니 나중에 시도하라고 나옴
- AWS EC2 탄력적 IP 가 제대로 활성화 및 연결 되어있지 않아서 인스턴스 정지 후 재 연결
- Gunicorn 경로가 잘못되어서 재설정
- 다시한번 certbot.letsencryp으로 시도 결과 성공
- HTTPS가 등록됨에 따라 구글 애드센스 광고 신청 완료

## 2. 배포환경 설정

- 기존 python manage.py runserver 0:8080으로 개발 서버 사용중이었는데
이제 gunicorn을 백그라운드로 실행해 putty(ssh) 창을 꺼도 서버가 실행됨 (https://namatji.com)

## 3. 추후 진행 예정
- **(중요)개인정보 이용 동의**
- 디자인 하기
- 데이터 전체 양식 변경
- elasticsearch 서버 관리 (과금, 정규서버 등 확인 핋요)
- 관리자 페이지 기능 추가
- 카카오톡 연동 및 다른 소셜 기능 추가
- 구글 애드센스 광고 위치 나오면 조절
- 웹버전 제작
- 아이콘 만들기?
- 쿠팡 파트너스 배너 넣기

<br>

# 2025.05.07 (수)

## 1. 쿠팡 파트너스 광고 배너 등 사이즈 조정
- 기존에 body 태그의 속성으로 margin 8px이 있어서 사이즈가 맞지 않아서 배너가 화면 밖으로 벗어나는 현상이 있었음 <br>
body 태그의 margin과 padding을 0으로 변경하고, <br>
광고 배너에 margin: 0 auto, width: 100%로 화면에 가로 크기가 딱 맞게 설정했음

## 2. 전문가 상담 페이지 - 개인정보 수집 동의
- 개인정보 수집 동의 체크박스 생성 및 유효성 검사 추가

## 3. 검색 조건 추가 및 해당하는 조건이 많은 순서로 출력

- 사업기간 : 실제 공고상에 제대로 명시되지 않은 경우가 대부분
- 매출액(매출규모) : 거의 다 무관, 공고마다 기준이 다름 ex) 수출금액, 업력, 카드 매출액 기준 등
- 직원 수 : 공고 내에 거의 안적혀있음

<br>

- open ai api의 프롬프트에서 사업기간, 매출규모 등 데이터의 양식을 지정해주었음 ex) 1년 미만, 1~3년 중 택1

<br>

# 2025.05.09 (금)

## 1. 기업마당 API

- 기업마당 API 호출
- 기업마당 지원사업 상세보기 페이지에서 viewer를 crawling하여 첨부파일 미리보기 <br>
chrome-driver와 chrome-application을 ubuntu에서 install하여 사용<br>
crawling은 selenium으로 js를 통해 동적으로 나타나는 viewer를 가져와서 beautyful soup 라이브러리를 통해 iframe태그를 가져와서 사용<br>
로컬 환경에선 뷰어가 잘 동작하는데 배포환경에선 잘 나타나지 않는 문제가 있었음, 기업마당 API 담당자에게 전화해서 문의해봤지만 그럴싸한 해결책은 나오지 않았음<br>
해결방법 : chrome-driver가 로컬환경에선 설치가 잘 되어있고 경로설정까지 해놨지만, 배포환경인 ubuntu에는 설치가 되어있지 않아서 문제가 있었다.<br>

- 네이버 서치 어드바이저, 구글 서치콘솔 여전히 인증 안됌
- 지원사업 공고 게시판, 상세보기 페이지에 홈으로 가는 버튼 추가 (root경로로 이동)
- 게시판, 상세보기에 쿠팡 파트너스 광고 배너 추가

## 2. 네이버 서치어드바이저, 구글 서치콘솔 등록

- 구글 서치콘솔에서 요청한 대로 가비아에서 DNS 설정을 해주었으나 자꾸 인증이 실패,<br>
가비아에 전화문의 해보니 가비아가 아닌 AWS에서 네임서버를 사용중이기 때문이라 했음<br>
AWS ROUTE53에서 레코드 설정을 통해 TXT타입의 DNS 설정해주니 인증 완료됌<br>

- 네이버 서치어드바이저의 다양한 인증 절차를 따라서 인증했으나 robots.txt가 인증이 안되었음<br>
내 서버에 robots.txt 및 sitemap.xml 설정을 통해 작성하고 등록하려했으나 인증이 안됌<br>
네이버 서치어드바이저에서 내 사이트의 robots.txt를 직접 인증해주니 등록 완료 되었음

## 3. 추후 진행
- API기반 데이터 구축 (DB에 저장)
- 저장된 API 데이터에 자체 제작 데이터셋 매칭
- 파일 뷰어(미리보기) 상세페이지에서 한번 더 들어가야 볼 수 있게 만들기

<br>

# 2025.05.12 (월)

## 1. 기업마당 API MySQL에 저장

- 기업마당 api 데이터 우리 서버 MySQL에 저장한 후 사용
- 데이터 매일 18시에 업데이트, pblancId 기준으로 저장, 최초 데이터 881개로 시작(4월 1일 공고부터)
  - linux의 crontab을 이용해서 등록된 시간에 해당 코드 (.py) 동작하도록 만듬

<br>

- 데이터 크롤링으로 기업마당 iframe태그의 src를 db에 같이 저장
- 약 700건 데이터 중 iframe 추출 못한 데이터 1개 발견(PBLN_000000000108483)
- 신청기간 데이터 없을 경우 상시 접수(예산 소진 시까지)
- 신청 양식이 없을 경우 if문으로 필터링해서 화면에 표시되지 않게함

## 2. 네이버, 구글 검색 유도

- url paturn을 path 방식으로 변경하여 site맵에 등록
- mata 태그를 상세페이지마다 등록되게 하여 공고명 검색 시 namatji의 결과를 robots가 추적할 수 있도록 만듬
- **네이버 서치 어드바이저, 구글 서치 콘솔에 sitemap.xml 재등록 필요**

# 2025.05.13 (화)

## 1. 웹 디자인 및 퍼블리싱
- 전체적인 색감과 버튼 디자인 변경
- 상단 네비게이션 바 추가 및 변경, 상단에 fixed
- 전문가 상담 버튼 우측 하단에 fixed
- 쿠팡 파트너스 배너 화면 하단 fixed
- 게시판 페이지 페이징처리 최대 10번까지, 이전, 다음버튼 추가, 맨처음, 맨끝 추가
- 게시판 카테고리에 지역 추가, 게시글 등록일 형식 YYYY-MM-dd에서 y.m.d로 변경
- 지역 카테고리에 소관부처 == 전국 으로 변경
- 지역은 두글자로 나오게 변경( ex 서울특별시 -> 서울)

## 2. 매일 18시마다 bizinfo api data update
- django crontab을 이용하여 매일 18시마다 코드 API 정보 받아오는 함수 호출 <br>
(* 18 * * *) - 순서대로 분, 시, 일, 월, 요일

---

## 데이터 파이프라인 구축

- 1. Bizinfo API 호출
- 2. 파일 다운로드 경로를 통해 파일 저장
- 3. 1- PDF 파일에서 텍스트 추출
- 3. 2- HWP 파일을 PNG 또는 PDF로 변경 (linux 환경에서는 기존처럼 쉽게 변경이 어려움)
- 3. 3- IMG 파일을 OCR을 통해 텍스트 추출
- 4. Open AI API를 통해 데이터 정형화 (매출액, 수출 여부 등)
- 5. Bizinfo 데이터와 직접 만든 데이터를 합쳐서 MySQL에 저장
- 6. 데이터 추출이 끝나면 해당 파일 삭제 (Ubuntu 서버 과부화 방지)

<br>

- 이전에 사용한 crontab 방식에 파이프라인 함수 추가 및 스케줄링
- multiprocessing | asyncio 등 병렬 처리로 속도 개선 고려
- logging | logfile.txt 등으로 로그 저장 및 실패 추적
- pblancId를 통해 중복 방지 및 캐싱

<br>

- 서버에서 완전 자동화를 위해 한글과 컴퓨터 등 windows 종속적인 방법은 배제
- Open AI API의 Function Calling을 통해 일관된 정형화
- 파일 사용 후 삭제로 서버 자원 보호

<br>
주의 사항

- 서버단에서 자동 실행이기 때문에 로그 추적 등 디버그가 간단하지 않음
- try - exeption으로 예외처리 등 확실하게 해주어야 함
- open ai를 통한 데이터 정형화 시 원하는 데이터를 출력하는지 확실히 검사하고 진행해야함

# 2025.05.14 (수)

## 1. 데이터 파이프라인 구축

- bizinfo API에서 각 데이터별 공고문 다운로드 링크를 통해 파일 다운로드
- libreoffice만으로는 hwp파일이 pdf로 변환하는데에 어려움이 있어서<br>
한글 폰트 설치 및 한글 지원 extention(libreoffice-h2orestart)을 통해 파일이 깨지지 않게 한 후 <br>
python에서 os.system으로 libreoffice의 convert to pdf 명령어 사용 <br>
(참고 : https://ubuntu.pkgs.org/24.04/ubuntu-universe-amd64/ibreoffice-h2orestart_0.6.1-1_all.deb.html)

<br>

- pdf파일의 경우 windows 환경에서 무난하게 진행되었고 ubuntu 환경에서도 문제없을 것이라 판단하였으나, <br>

  windows에서는 유도리있게 확장자가 pdf라면 알아서 이미지든 html이든 텍스트 추출이 가능했으나, <br>

  linux에서는 파일을 까다롭게 확인하여 .pdf 파일 내부에 /Root 객체가 없는 경우에 pdf로 판단하지 않는 문제가 있었음 (error: No /Root object! – Is this really a PDF?)<br>
  
  파일의 첫 바이트 magic number를 검사하여 텍스트형식 pdf인지 이미지형 pdf거나 구조가 비정상적인 경우를 파악하여 텍스트형 pdf일 경우 기존 방식(pdfplumber)대로 진행, 이미지형 pdf의 경우 naver CLOVA OCR을 통해 텍스트 추출하는 방식으로 해결하였다.

# 2025.05.15 (목)

## 1. open AI API prompt 조정
- bizinfo api의 데이터와 우리 양식으로 만든 데이터를 합치는 것은 성공하였으나, gpt의 응답이 대부분 무관인 점을 확인.<br>
프롬프트 조절을 통해 데이터를 최대한 지원사업의 선정 기준에 부합하도록 설정

- gpt의 응답 중 특히 업종이 부정확한 경우가 많은 점을 발견하였고, 이를 해결하기 위해 서비스업을 전문서비스, 생활 서비스 둘로 나누고 전체 공고문이 아닌 공고 내용 요약에서 업종을 판별하도록 하였다.
- 수출 여부또한 수출 희망, 수출 기업 뿐이라 수출 없음 추가

## 2. SEO (Search Engin Optimazing, 검색엔진 최적화)
- 한 페이지당 한개의 h1태그를 이용해 해당 페이지의 제목, 중점 내용 등을 기재 (robots등이 사이트 크롤링 후 내용 파악에 h1태그를 중요하게 본다.)
- 시맨틱 태그를 이용하여 봇이 html의 구조파악을 쉽게 하도록 도와줌 ex) <header>, <section> 등

## 3. Cron 자동화 재설정
- crontab이 자꾸 실행되지 않아 기존 command 내의 update_bizinfo.py 파일을 바로 실행하는 방식에서 <br>
update_bizinfo.sh 의 스크립트를 통해 실행하는 방식으로 바꾸었다.